{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b6b1e083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TextBlob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\pichau\\anaconda3\\lib\\site-packages (from TextBlob) (3.6.1)\n",
      "Requirement already satisfied: regex in c:\\users\\pichau\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (2021.4.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\pichau\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\pichau\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pichau\\anaconda3\\lib\\site-packages (from nltk>=3.1->TextBlob) (4.59.0)\n",
      "Installing collected packages: TextBlob\n",
      "Successfully installed TextBlob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b8d10a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word      PoS polarity polarity_target polarity_classification\n",
      "0       a-vontade    PoS=N        1              N0                     MAN\n",
      "1         abafada  PoS=Adj      NaN              N0                    JALC\n",
      "2        abafadas  PoS=Adj      NaN              N0                    JALC\n",
      "3         abafado  PoS=Adj      NaN              N0                    JALC\n",
      "4        abafados  PoS=Adj      NaN              N0                    JALC\n",
      "...           ...      ...      ...             ...                     ...\n",
      "164689       zote  PoS=Adj      NaN              N0                     MAN\n",
      "164690   zumbidor  PoS=Adj      NaN              N0                     MAN\n",
      "164691   zumbidor  PoS=Adj      NaN              N0                     MAN\n",
      "164692   zumbidor  PoS=Adj      NaN              N0                     MAN\n",
      "164693   zumbidor  PoS=Adj      NaN              N0                     MAN\n",
      "\n",
      "[164694 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "#sentilex = open('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\SentiLex-flex-PT02.txt', 'r', encoding='utf8')\n",
    "\n",
    "# Read Sentilex-lem-PT02\n",
    "sentilex_lem = pd.read_csv('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\SentiLex-flex-PT02.txt', sep=',', header=None, names=['term', 'definition'])\n",
    "sentilex_lem[['term2','definition_c']] = sentilex_lem['definition'].str.split('.', expand=True)\n",
    "# Split second column into multiple columns\n",
    "sentilex_lem[['PoS', 'FLEX', 'TG', 'POL', 'ANOT','vazio']] = sentilex_lem['definition_c'].str.split(';', expand=True)\n",
    "\n",
    "# Remove REV if present\n",
    "sentilex_lem['POL'] = sentilex_lem['POL'].apply(lambda x: re.sub('REV=[0-9]*:', '', x))\n",
    "\n",
    "# Remove POL:N1 only if POL:N0 is not present in the vector\n",
    "def clean_N0N1(x):\n",
    "    if 'POL:N1' in x and 'POL:N0' not in x:\n",
    "        x.remove('POL:N1')\n",
    "    return x\n",
    "\n",
    "sentilex_lem['POL'] = sentilex_lem['POL'].str.split(':').apply(clean_N0N1).apply(':'.join)\n",
    "\n",
    "\n",
    "# Extract polarity from POL\n",
    "sentilex_lem['polarity'] = sentilex_lem['POL'].str.extract(r'POL:N0=(\\d+)')\n",
    "\n",
    "\n",
    "# Extract target from TG\n",
    "sentilex_lem['polarity_target'] = sentilex_lem['TG'].apply(lambda x: x.split(':')[1] if x.startswith('TG=HUM') else x)\n",
    "\n",
    "# Extract polarity classification from ANOT\n",
    "sentilex_lem['polarity_classification'] = sentilex_lem['ANOT'].str.extract(r'ANOT=(\\w+)')\n",
    "\n",
    "# Remove unnecessary columns\n",
    "sentilex_lem.drop(['definition','definition_c', 'FLEX', 'TG', 'POL', 'ANOT','vazio'], axis=1, inplace=True)\n",
    "\n",
    "# Fix encoding and remove non-ASCII strings\n",
    "def remove_non_ascii(text):\n",
    "    return ''.join(char for char in unicodedata.normalize('NFKD', text) if unicodedata.category(char) != 'Mn')\n",
    "\n",
    "sentilex_lem['term'] = sentilex_lem['term'].apply(lambda x: remove_non_ascii(x)).str.encode('ascii', 'ignore').str.decode('utf-8')\n",
    "\n",
    "sentilex_lem['term2'] = sentilex_lem['term2'].apply(lambda x: remove_non_ascii(x)).str.encode('ascii', 'ignore').str.decode('utf-8')\n",
    "\n",
    "\n",
    "sentilex_lem\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Melt the dataframe to create a new column for stacked values of A and B\n",
    "melted_df = sentilex_lem.melt(id_vars=['PoS','polarity', 'polarity_target','polarity_classification'], value_vars=['term', 'term2'], var_name='term3')\n",
    "\n",
    "# Create a new dataframe with the stacked values and replicated C and D columns\n",
    "new_df = pd.DataFrame({\n",
    "    'word': melted_df['value'],\n",
    "    'PoS': melted_df['PoS'],\n",
    "    'polarity': melted_df['polarity'],\n",
    "    'polarity_target': melted_df['polarity_target'],\n",
    "    'polarity_classification': melted_df['polarity_classification']\n",
    "    \n",
    "})\n",
    "\n",
    "# Print the new dataframe\n",
    "print(new_df)\n",
    "senti_lex = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f0830467",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 1 elements, new values have 3 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-37c6ea26da1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Load Lexico v3.0 dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mlexico\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\lexico_v2.1txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mlexico\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'word'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'polarity'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Load SentiLex-flex-PT02 dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   5476\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5477\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5478\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5479\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5480\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    221\u001b[0m                 \u001b[1;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[1;34mf\"values have {new_len} elements\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 1 elements, new values have 3 elements"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load Lexico v3.0 dictionary\n",
    "lexico = pd.read_csv('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\lexico_v2.1txt', sep='\\t', header=None)\n",
    "lexico.columns = ['word', 'polarity', 'sentiment']\n",
    "\n",
    "# Load SentiLex-flex-PT02 dictionary\n",
    "#senti_lex = pd.read_csv('SentiLex-flex-PT02.txt', sep='\\t', header=None)\n",
    "#senti_lex.columns = ['word', 'PoS', 'polarity', 'polarity_target', 'polarity_classification']\n",
    "\n",
    "# Load Vader sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Read in PDF file\n",
    "pdf_file = open('Release de Resultados 1T22.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "\n",
    "# Tokenize PDF text into sentences\n",
    "text = ''\n",
    "for i in range(pdf_reader.getNumPages()):\n",
    "    page = pdf_reader.getPage(i)\n",
    "    text += page.extractText()\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Define sentiment score functions\n",
    "def lexico_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in lexico['word'].values:\n",
    "            score += lexico.loc[lexico['word'] == token, 'sentiment'].values[0]\n",
    "    return score\n",
    "\n",
    "def sentilex_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in senti_lex['word'].values:\n",
    "            score += senti_lex.loc[senti_lex['word'] == token, 'polarity'].values[0]\n",
    "    return score\n",
    "\n",
    "# Compute sentiment scores for each sentence\n",
    "lexico_scores = [lexico_score(sentence) for sentence in sentences]\n",
    "sentilex_scores = [sentilex_score(sentence) for sentence in sentences]\n",
    "vader_scores = [vader.polarity_scores(sentence)['compound'] for sentence in sentences]\n",
    "textblob_scores = [TextBlob(sentence).sentiment.polarity for sentence in sentences]\n",
    "\n",
    "# Compare the scores\n",
    "for i in range(len(sentences)):\n",
    "    print(f\"Sentence {i}:\")\n",
    "    print(f\"  Lexico: {lexico_scores[i]}\")\n",
    "    print(f\"  SentiLex: {sentilex_scores[i]}\")\n",
    "    print(f\"  Vader: {vader_scores[i]}\")\n",
    "    print(f\"  TextBlob: {textblob_scores[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
