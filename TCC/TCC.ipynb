{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b6b1e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SentiLex-flex-PT02 dictionary\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "file = open('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\SentiLex-lem-PT02.txt', 'r', encoding='utf-8')\n",
    "\n",
    "lines = []\n",
    "\n",
    "for line in file:\n",
    "    lines.append(line.strip('.'))\n",
    "    \n",
    "file.close()\n",
    "\n",
    "data = {'word':[],'PoS': [],  'polarity_target': [], 'polarity': [], 'polarity_classification': []}\n",
    "\n",
    "for line in lines:\n",
    "    columns = re.split(r'[.;]', line)\n",
    "    data['word'].append(columns[0])\n",
    "    data['PoS'].append(columns[1].split('=')[1])\n",
    "    #data['FLEX'].append(columns[2].split('=')[1])\n",
    "    data['polarity_target'].append(columns[2].split('=')[1])\n",
    "    data['polarity'].append(columns[3].split('=')[1])\n",
    "    data['polarity_classification'].append(columns[4].split('=')[1])\n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df['polarity'] = pd.to_numeric(df['polarity'])\n",
    "\n",
    "senti_lex = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f0830467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results:\n",
      "Lexico Avg Score: -0.2846153846153846\n",
      "Sentilex Avg Score: 0.13846153846153847\n",
      "OPLexicon Avg Score: -0.36153846153846153\n",
      "Vader Avg Score: -0.09990384615384612\n",
      "TextBlob Avg Score: 0.024532967032967024\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# Load Lexico v2.0 dictionary\n",
    "lexico = pd.read_csv('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\lexico_v2.1txt', sep=',', header=None)\n",
    "lexico.columns = ['word', 'polarity', 'sentiment']\n",
    "\n",
    "# Load OPLexico v3.0 dictionary\n",
    "oplexico = pd.read_csv('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\oplexicon_v3.0\\\\lexico_v3.0.txt', sep=',', header=None)\n",
    "oplexico.columns = ['word', 'polarity', 'sentiment','classification']\n",
    "\n",
    "# Load SentiLex-flex-PT02 dictionary\n",
    "\n",
    "file = open('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\SentiLex-lem-PT02.txt', 'r', encoding='utf-8')\n",
    "lines = []\n",
    "for line in file:\n",
    "    lines.append(line.strip('.'))  \n",
    "file.close()\n",
    "data = {'word':[],'PoS': [],  'polarity_target': [], 'polarity': [], 'polarity_classification': []}\n",
    "for line in lines:\n",
    "    columns = re.split(r'[.;]', line)\n",
    "    data['word'].append(columns[0])\n",
    "    data['PoS'].append(columns[1].split('=')[1])\n",
    "    #data['FLEX'].append(columns[2].split('=')[1])\n",
    "    data['polarity_target'].append(columns[2].split('=')[1])\n",
    "    data['polarity'].append(columns[3].split('=')[1])\n",
    "    data['polarity_classification'].append(columns[4].split('=')[1])\n",
    "   \n",
    "df = pd.DataFrame(data)\n",
    "df['polarity'] = pd.to_numeric(df['polarity'])\n",
    "senti_lex = df\n",
    "\n",
    "# Load Vader sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Read in PDF file\n",
    "pdf_file = open('Release de Resultados 1T22.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "\n",
    "# Tokenize PDF text into sentences\n",
    "\n",
    "text = ''\n",
    "for i in range(pdf_reader.getNumPages()):\n",
    "    page = pdf_reader.getPage(i)\n",
    "    text += page.extractText()\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Define sentiment score functions\n",
    "def lexico_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in lexico['word'].values:\n",
    "            score += lexico.loc[lexico['word'] == token, 'sentiment'].values[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "#def oplexicon_score(sentence):\n",
    "#    oplexicon_score = 0\n",
    "#    for word in sentence.split():\n",
    "#        if word.lower() in oplexico['word'].values:\n",
    "#            oplexicon_score += oplexico.loc[(oplexico['word'] == word.lower()), 'sentiment'].values[0]\n",
    "#    return oplexicon_score   \n",
    "\n",
    "\n",
    "def oplexicon_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in oplexico['word'].values:\n",
    "            score += oplexico.loc[oplexico['word'] == token, 'sentiment'].values[0]\n",
    "    return score\n",
    "\n",
    "def sentilex_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in senti_lex['word'].values:\n",
    "            score += senti_lex.loc[senti_lex['word'] == token, 'polarity'].values[0]\n",
    "    return score\n",
    "\n",
    "# Compute sentiment scores for each sentence\n",
    "lexico_scores = [lexico_score(sentence) for sentence in sentences]\n",
    "sentilex_scores = [sentilex_score(sentence) for sentence in sentences]\n",
    "oplexicon_score = [oplexicon_score(sentence) for sentence in sentences]\n",
    "vader_scores = [vader.polarity_scores(sentence)['compound'] for sentence in sentences]\n",
    "textblob_scores = [TextBlob(sentence).sentiment.polarity for sentence in sentences]\n",
    "\n",
    "\n",
    "results = []\n",
    "# Compare the scores\n",
    "for i in range(len(sentences)):\n",
    "    result = {\n",
    "            'sentence': i,\n",
    "            'vader_score': vader_scores[i],\n",
    "            'lexico_score': lexico_scores[i],\n",
    "            'sentilex_score': sentilex_scores[i],\n",
    "            'oplexicon_score': oplexicon_score[i],\n",
    "            'textblob_score': textblob_scores[i]\n",
    "        }\n",
    "    results.append(result)\n",
    "    \n",
    "# Compute total sentiment scores for the document\n",
    "lexico_total = sum(lexico_scores)\n",
    "sentilex_total = sum(sentilex_scores)\n",
    "oplexicon_total = sum(oplexicon_score)\n",
    "vader_total = sum(vader_scores)\n",
    "textblob_total = sum(textblob_scores)\n",
    "\n",
    "# Compute average sentiment scores for the document\n",
    "num_sentences = len(sentences)\n",
    "lexico_avg = lexico_total / num_sentences\n",
    "sentilex_avg = sentilex_total / num_sentences\n",
    "oplexicon_avg = oplexicon_total / num_sentences\n",
    "vader_avg = vader_total / num_sentences\n",
    "textblob_avg = textblob_total / num_sentences\n",
    "\n",
    "# Print the final results\n",
    "print(\"Final Results:\")\n",
    "print(f\"Lexico Avg Score: {lexico_avg}\")\n",
    "print(f\"Sentilex Avg Score: {sentilex_avg}\")\n",
    "print(f\"OPLexicon Avg Score: {oplexicon_avg}\")\n",
    "print(f\"Vader Avg Score: {vader_avg}\")\n",
    "print(f\"TextBlob Avg Score: {textblob_avg}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5c8affde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3111184615384615"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_file = open('Release de Resultados 1T22.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "\n",
    "# Tokenize PDF text into sentences\n",
    "text = ''\n",
    "for i in range(pdf_reader.getNumPages()):\n",
    "    page = pdf_reader.getPage(i)\n",
    "    text += page.extractText()\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Define sentiment score functions\n",
    "def lexico_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in lexico['word'].values:\n",
    "            score += lexico.loc[lexico['word'] == token, 'sentiment'].values[0]\n",
    "    return score\n",
    "\n",
    "def oplexicon_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in oplexico['word'].values:\n",
    "            score += oplexico.loc[oplexico['word'] == token, 'sentiment'].values[0]\n",
    "    return score\n",
    "\n",
    "def sentilex_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in senti_lex['word'].values:\n",
    "            score += senti_lex.loc[senti_lex['word'] == token, 'polarity'].values[0]\n",
    "    return score\n",
    "\n",
    "def vader_score(sentence):\n",
    "    return vader.polarity_scores(sentence)['compound']\n",
    "\n",
    "def textblob_score(sentence):\n",
    "    return TextBlob(sentence).sentiment.polarity\n",
    "\n",
    "\n",
    "# Compute sentiment scores for each sentence\n",
    "results = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # First, try to use the lexico_score\n",
    "    score = lexico_score(sentence)\n",
    "    source = 'lexico_score'\n",
    "    \n",
    "    # If the score is 0, try to use the oplexicon_score\n",
    "    if score == 0:\n",
    "        score = oplexicon_score(sentence)\n",
    "        source = 'oplexicon_score'\n",
    "        \n",
    "    # If the score is 0, try to use the sentilex_score\n",
    "    if score == 0:\n",
    "        score = sentilex_score(sentence)\n",
    "        source = 'sentilex_score'\n",
    "    \n",
    "    # If the score is 0, use the Vader score\n",
    "    if score == 0:\n",
    "        score = vader.polarity_scores(sentence)['compound']\n",
    "        source = 'vader_score'\n",
    "    \n",
    "    # If the score is still 0, use the TextBlob score\n",
    "    if score == 0:\n",
    "        score = TextBlob(sentence).sentiment.polarity\n",
    "        source = 'textblob_score'\n",
    "    \n",
    "    result = {\n",
    "        'sentence': i,\n",
    "        'score': score,\n",
    "        'source': source,\n",
    "        'text': sentence,\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "    \n",
    "# Convert results to a pandas DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n",
    "\n",
    "# Compute overall sentiment score\n",
    "overall_score = results_df['score'].mean()\n",
    "overall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "41e62331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results:\n",
      "Lexico Avg Score: -0.2846153846153846\n",
      "Sentilex Avg Score: 0.13846153846153847\n",
      "OPLexicon Avg Score: -0.36153846153846153\n",
      "Vader Avg Score: -0.09990384615384612\n",
      "TextBlob Avg Score: 0.024532967032967024\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import PyPDF2\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "# Load Lexico v2.0 dictionary\n",
    "lexico = pd.read_csv('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\lexico_v2.1txt', sep=',', header=None)\n",
    "lexico.columns = ['word', 'polarity', 'sentiment']\n",
    "\n",
    "# Load OPLexico v3.0 dictionary\n",
    "oplexico = pd.read_csv('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\oplexicon_v3.0\\\\lexico_v3.0.txt', sep=',', header=None)\n",
    "oplexico.columns = ['word', 'polarity', 'sentiment','classification']\n",
    "\n",
    "# Load SentiLex-flex-PT02 dictionary\n",
    "\n",
    "file = open('C:\\\\Users\\\\PICHAU\\\\Documents\\\\ArthurAnzai\\\\lexiconPT\\\\data-raw\\\\SentiLex-lem-PT02.txt', 'r', encoding='utf-8')\n",
    "\n",
    "lines = []\n",
    "\n",
    "for line in file:\n",
    "    lines.append(line.strip('.'))  \n",
    "file.close()\n",
    "data = {'word':[],'PoS': [],  'polarity_target': [], 'polarity': [], 'polarity_classification': []}\n",
    "for line in lines:\n",
    "    columns = re.split(r'[.;]', line)\n",
    "    data['word'].append(columns[0])\n",
    "    data['PoS'].append(columns[1].split('=')[1])\n",
    "    #data['FLEX'].append(columns[2].split('=')[1])\n",
    "    data['polarity_target'].append(columns[2].split('=')[1])\n",
    "    data['polarity'].append(columns[3].split('=')[1])\n",
    "    data['polarity_classification'].append(columns[4].split('=')[1])\n",
    "   \n",
    "df = pd.DataFrame(data)\n",
    "df['polarity'] = pd.to_numeric(df['polarity'])\n",
    "senti_lex = df\n",
    "\n",
    "# Load Vader sentiment analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define sentiment score functions\n",
    "def lexico_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in lexico['word'].values:\n",
    "            score += lexico.loc[lexico['word'] == token, 'sentiment'].values[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "def oplexicon_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in oplexico['word'].values:\n",
    "            score += oplexico.loc[oplexico['word'] == token, 'sentiment'].values[0]\n",
    "    return score\n",
    "\n",
    "\n",
    "def sentilex_score(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in senti_lex['word'].values:\n",
    "            score += senti_lex.loc[senti_lex['word'] == token, 'polarity'].values[0]\n",
    "    return score\n",
    "\n",
    "def sentiment_analysis_sentence(pdf):\n",
    "    # Read in PDF file\n",
    "    pdf_file = open(pdf, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "\n",
    "    # Tokenize PDF text into sentences\n",
    "\n",
    "    text = ''\n",
    "    for i in range(pdf_reader.getNumPages()):\n",
    "        page = pdf_reader.getPage(i)\n",
    "        text += page.extractText()\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Compute sentiment scores for each sentence\n",
    "    lexico_scores = [lexico_score(sentence) for sentence in sentences]\n",
    "    sentilex_scores = [sentilex_score(sentence) for sentence in sentences]\n",
    "    oplexicon_scores = [oplexicon_score(sentence) for sentence in sentences]\n",
    "    vader_scores = [vader.polarity_scores(sentence)['compound'] for sentence in sentences]\n",
    "    textblob_scores = [TextBlob(sentence).sentiment.polarity for sentence in sentences]\n",
    "\n",
    "\n",
    "    results = []\n",
    "    # Compare the scores\n",
    "    for i in range(len(sentences)):\n",
    "        result = {\n",
    "                'sentence': i,\n",
    "                'vader_score': vader_scores[i],\n",
    "                'lexico_score': lexico_scores[i],\n",
    "                'sentilex_score': sentilex_scores[i],\n",
    "                'oplexicon_score': oplexicon_scores[i],\n",
    "                'textblob_score': textblob_scores[i]\n",
    "            }\n",
    "        results.append(result)\n",
    "\n",
    "    # Compute total sentiment scores for the document\n",
    "    lexico_total = sum(lexico_scores)\n",
    "    sentilex_total = sum(sentilex_scores)\n",
    "    oplexicon_total = sum(oplexicon_scores)\n",
    "    vader_total = sum(vader_scores)\n",
    "    textblob_total = sum(textblob_scores)\n",
    "\n",
    "    # Compute average sentiment scores for the document\n",
    "    num_sentences = len(sentences)\n",
    "    lexico_avg = lexico_total / num_sentences\n",
    "    sentilex_avg = sentilex_total / num_sentences\n",
    "    oplexicon_avg = oplexicon_total / num_sentences\n",
    "    vader_avg = vader_total / num_sentences\n",
    "    textblob_avg = textblob_total / num_sentences\n",
    "    \n",
    "    # Print the final results\n",
    "    print(\"Final Results:\")\n",
    "    print(f\"Lexico Avg Score: {lexico_avg}\")\n",
    "    print(f\"Sentilex Avg Score: {sentilex_avg}\")\n",
    "    print(f\"OPLexicon Avg Score: {oplexicon_avg}\")\n",
    "    print(f\"Vader Avg Score: {vader_avg}\")\n",
    "    print(f\"TextBlob Avg Score: {textblob_avg}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "5c27a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentiment_analysis_overall(pdf):\n",
    "    \n",
    "    pdf_file = open('Release de Resultados 1T22.pdf', 'rb')\n",
    "    pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "\n",
    "    # Tokenize PDF text into sentences\n",
    "    text = ''\n",
    "    for i in range(pdf_reader.getNumPages()):\n",
    "        page = pdf_reader.getPage(i)\n",
    "        text += page.extractText()\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Compute sentiment scores for each sentence\n",
    "    results = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # First, try to use the lexico_score\n",
    "        score = lexico_score(sentence)\n",
    "        source = 'lexico_score'\n",
    "\n",
    "        # If the score is 0, try to use the oplexicon_score\n",
    "        if score == 0:\n",
    "            score = oplexicon_score(sentence)\n",
    "            source = 'oplexicon_score'\n",
    "\n",
    "        # If the score is 0, try to use the sentilex_score\n",
    "        if score == 0:\n",
    "            score = sentilex_score(sentence)\n",
    "            source = 'sentilex_score'\n",
    "\n",
    "        # If the score is 0, use the Vader score\n",
    "        if score == 0:\n",
    "            score = vader.polarity_scores(sentence)['compound']\n",
    "            source = 'vader_score'\n",
    "\n",
    "        # If the score is still 0, use the TextBlob score\n",
    "        if score == 0:\n",
    "            score = TextBlob(sentence).sentiment.polarity\n",
    "            source = 'textblob_score'\n",
    "\n",
    "        result = {\n",
    "            'sentence': i,\n",
    "            'score': score,\n",
    "            'source': source,\n",
    "            'text': sentence,\n",
    "        }\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    # Convert results to a pandas DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df\n",
    "\n",
    "    # Compute overall sentiment score\n",
    "    overall_score = results_df['score'].mean()\n",
    "    overall_score\n",
    "    print(f\"Overall Sentiment Score: {overall_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8075a9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results:\n",
      "Lexico Avg Score: -0.2846153846153846\n",
      "Sentilex Avg Score: 0.13846153846153847\n",
      "OPLexicon Avg Score: -0.36153846153846153\n",
      "Vader Avg Score: -0.09990384615384612\n",
      "TextBlob Avg Score: 0.024532967032967024\n",
      "Overall Sentiment Score: -0.3111184615384615\n"
     ]
    }
   ],
   "source": [
    "sentiment_analysis_sentence('Release de Resultados 1T22.pdf')\n",
    "sentiment_analysis_overall('Release de Resultados 1T22.pdf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
